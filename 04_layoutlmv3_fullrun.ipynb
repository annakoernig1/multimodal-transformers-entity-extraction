{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiment Run â€“ LayoutLMv3 on FUNSD\n",
    "\n",
    "## Purpose\n",
    "This notebook executes the **final training and evaluation run** of **LayoutLMv3** on the **full FUNSD dataset**.\n",
    "The experimental setup is designed to be **comparable to the LayoutLM baseline**:\n",
    "- same dataset split strategy (train/test)\n",
    "- same evaluation metric (entity-level Precision/Recall/F1 using seqeval)\n",
    "- same random seed and training procedure (fine-tuning, Trainer-based)\n",
    "\n",
    "## Key difference to LayoutLM\n",
    "LayoutLMv3 incorporates **visual information** (document images) in addition to text and layout.\n",
    "Therefore, inputs include `pixel_values` besides `input_ids`, `bbox`, and `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b6562",
   "metadata": {},
   "source": [
    "## Imports and Reproducibility\n",
    "We fix random seeds to improve reproducibility and disable tokenizers parallelism to avoid notebook deadlocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f5b161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ba6eed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8955706",
   "metadata": {},
   "source": [
    "## Full-Run Configuration (Comparable to LayoutLM)\n",
    "To ensure comparability, we use:\n",
    "- full train/test splits\n",
    "- batch size = 1 (CPU-only environment)\n",
    "- fixed number of epochs (e.g., 3)\n",
    "- identical seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2895edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-run parameters (match your LayoutLM baseline as closely as possible)\n",
    "epochs = 3\n",
    "batch_size = 1\n",
    "\n",
    "dataset_name = \"nielsr/funsd-layoutlmv3\"\n",
    "model_name = \"microsoft/layoutlmv3-base\"\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(\"results\", \"funsd\", \"layoutlmv3\", \"fullrun\", run_id)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "run_config = {\n",
    "    \"dataset\": dataset_name,\n",
    "    \"model\": model_name,\n",
    "    \"train_split\": \"train\",\n",
    "    \"eval_split\": \"test\",\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": seed,\n",
    "    \"device\": \"cpu\",\n",
    "    \"run_id\": run_id,\n",
    "    \"output_dir\": output_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c27b26",
   "metadata": {},
   "source": [
    "## Dataset Loading and Label Space\n",
    "We load the LayoutLMv3-ready FUNSD dataset variant and extract the BIO label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa58f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'bboxes', 'ner_tags', 'image'],\n",
      "        num_rows: 149\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'bboxes', 'ner_tags', 'image'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "Sample keys: ['id', 'tokens', 'bboxes', 'ner_tags', 'image']\n",
      "Using token field: tokens\n",
      "Number of labels: 7\n",
      "First labels: ['O', 'B-HEADER', 'I-HEADER', 'B-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER']\n"
     ]
    }
   ],
   "source": [
    "funsd = load_dataset(dataset_name)\n",
    "print(funsd)\n",
    "\n",
    "train_split = \"train\"\n",
    "eval_split = \"test\"\n",
    "\n",
    "train_raw = funsd[train_split]\n",
    "eval_raw = funsd[eval_split]\n",
    "\n",
    "run_config[\"train_split_size\"] = len(train_raw)\n",
    "run_config[\"eval_split_size\"] = len(eval_raw)\n",
    "\n",
    "sample = train_raw[0]\n",
    "print(\"Sample keys:\", list(sample.keys()))\n",
    "\n",
    "token_field = \"words\" if \"words\" in sample else (\"tokens\" if \"tokens\" in sample else None)\n",
    "if token_field is None:\n",
    "    raise KeyError(\"Expected 'words' or 'tokens' field in the dataset sample.\")\n",
    "print(\"Using token field:\", token_field)\n",
    "\n",
    "# Label list (BIO tags)\n",
    "if \"ner_tags\" in funsd[train_split].features:\n",
    "    label_list = funsd[train_split].features[\"ner_tags\"].feature.names\n",
    "else:\n",
    "    raise KeyError(\"Expected 'ner_tags' in dataset features.\")\n",
    "num_labels = len(label_list)\n",
    "\n",
    "run_config[\"num_labels\"] = num_labels\n",
    "run_config[\"label_list_preview\"] = label_list[:10]\n",
    "\n",
    "print(\"Number of labels:\", num_labels)\n",
    "print(\"First labels:\", label_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800f02d",
   "metadata": {},
   "source": [
    "## Processor and Model Initialization\n",
    "LayoutLMv3 requires a processor (tokenizer + image processor).  \n",
    "We explicitly set `apply_ocr=False` because we provide **existing bounding boxes** from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26e43c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakoernig/multimodal-document-ai-evaluation/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(model_name, apply_ocr=False)\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# Label mappings for clean logs/analysis\n",
    "id2label = {i: lab for i, lab in enumerate(label_list)}\n",
    "label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be8a6d",
   "metadata": {},
   "source": [
    "## Encoding (Text + Layout + Image)\n",
    "To make batching stable (especially for `pixel_values`), we **pad to a fixed max length** (512).  \n",
    "Labels are aligned to tokens such that only the **first subtoken** of each word receives a label; subsequent subtokens are set to `-100`. This is required for correct entity-level evaluation with seqeval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50764208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_funsd_examples_layoutlmv3(raw_ds, processor, token_field, max_length=512):\n",
    "    encoded = []\n",
    "\n",
    "    for example in raw_ds:\n",
    "        words = example[token_field]\n",
    "\n",
    "        # bboxes key can differ across dataset variants\n",
    "        word_boxes = example[\"bboxes\"] if \"bboxes\" in example else example[\"bbox\"]\n",
    "\n",
    "        # labels key (usually ner_tags)\n",
    "        word_labels = example[\"ner_tags\"] if \"ner_tags\" in example else example[\"labels\"]\n",
    "\n",
    "        if \"image\" not in example:\n",
    "            raise KeyError(\"Expected an 'image' field for LayoutLMv3 inputs.\")\n",
    "        image = example[\"image\"]\n",
    "\n",
    "        # Processor combines image + tokens + boxes\n",
    "        encoding = processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=word_boxes,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"np\",\n",
    "        )\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"][0]\n",
    "        attention_mask = encoding[\"attention_mask\"][0]\n",
    "        bbox = encoding[\"bbox\"][0]\n",
    "        pixel_values = encoding[\"pixel_values\"][0]\n",
    "\n",
    "        # Obtain word_ids (LayoutLMv3 tokenizer requires boxes if called separately)\n",
    "        try:\n",
    "            word_ids = encoding.word_ids(batch_index=0)\n",
    "        except Exception:\n",
    "            tok = processor.tokenizer(\n",
    "                words,\n",
    "                boxes=word_boxes,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                is_split_into_words=True,\n",
    "            )\n",
    "            word_ids = tok.word_ids()\n",
    "\n",
    "        labels = []\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                if word_id != previous_word_id:\n",
    "                    labels.append(int(word_labels[word_id]))\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"bbox\": bbox,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": np.array(labels, dtype=np.int64),\n",
    "        }\n",
    "        encoded.append(item)\n",
    "\n",
    "    return Dataset.from_list(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd8361",
   "metadata": {},
   "source": [
    "## Build Torch Datasets\n",
    "We encode the full train and test splits and convert them to torch tensors for Trainer compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8649fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 149\n",
      "Eval size: 50\n",
      "512 512\n",
      "Example keys: dict_keys(['input_ids', 'attention_mask', 'bbox', 'pixel_values', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = encode_funsd_examples_layoutlmv3(\n",
    "    train_raw, processor=processor, token_field=token_field, max_length=512\n",
    ")\n",
    "eval_dataset = encode_funsd_examples_layoutlmv3(\n",
    "    eval_raw, processor=processor, token_field=token_field, max_length=512\n",
    ")\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "\n",
    "# sanity check: equal lengths after max_length padding\n",
    "print(len(train_dataset[0][\"input_ids\"]), len(train_dataset[1][\"input_ids\"]))\n",
    "print(\"Example keys:\", train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b860bc",
   "metadata": {},
   "source": [
    "## Entity-level Evaluation (seqeval)\n",
    "We report entity-level Precision/Recall/F1 based on BIO tags.  \n",
    "Tokens with label `-100` are ignored (special tokens and non-first subtokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a52a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            l = int(l)\n",
    "            if l == -100:\n",
    "                continue\n",
    "            p = int(p)\n",
    "            seq_true.append(label_list[l])\n",
    "            seq_pred.append(label_list[p])\n",
    "\n",
    "        if len(seq_true) > 0:\n",
    "            true_labels.append(seq_true)\n",
    "            true_preds.append(seq_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88855809",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "We fine-tune LayoutLMv3 on CPU using the HuggingFace Trainer API.  \n",
    "To keep the run stable in notebooks, we set `dataloader_num_workers=0` and disable checkpoint saving.\n",
    "Results are evaluated on the test split after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    seed=seed,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    use_cpu=True,\n",
    "\n",
    "    # Explicit hyperparams (optional, but good for reproducibility)\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# For LayoutLMv3 with fixed-length inputs, default_data_collator is robust (stacks pixel_values cleanly)\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "print(\"Train result:\", train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bdd11",
   "metadata": {},
   "source": [
    "## Saving Results for Reproducibility\n",
    "We store the run configuration and evaluation metrics as JSON files.\n",
    "This supports transparent reporting and later comparison with LayoutLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da47235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, \"run_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "\n",
    "print(f\"Saved run_config.json and metrics.json to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
