{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiment Run – LayoutLM on FUNSD\n",
    "\n",
    "## Purpose\n",
    "This notebook executes the **final training and evaluation run** of the LayoutLM model on the **full FUNSD dataset**.\n",
    "The underlying preprocessing pipeline, model configuration, and evaluation procedure were previously validated\n",
    "in a separate **dry-run notebook** using reduced dataset subsets.\n",
    "\n",
    "## Experimental Setup\n",
    "- **Model:** LayoutLM (microsoft/layoutlm-base-uncased)\n",
    "- **Dataset:** FUNSD\n",
    "- **Task:** Token Classification (entity-level evaluation using BIO tags)\n",
    "- **Training Mode:** Fine-tuning (CPU-only)\n",
    "- **Evaluation:** Entity-level Precision / Recall / F1 (seqeval)\n",
    "- **Seed:** 42\n",
    "\n",
    "## Notes\n",
    "- This notebook differs from the dry-run **only in runtime parameters** (dataset size, number of epochs).\n",
    "- No changes were made to the model architecture, preprocessing, or evaluation logic.\n",
    "- All results and configurations are stored in a dedicated output directory to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f5b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    LayoutLMTokenizerFast,\n",
    "    LayoutLMForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# Entity-level (NER-span) Metrics\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Notebook-Stabilität ---\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# --- Reproduzierbarkeit ---\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# =========================\n",
    "# Full Run Konfiguration\n",
    "# =========================\n",
    "\n",
    "epochs = 3          # Empfehlung für finalen Run (alternativ 2 bei Zeitdruck)\n",
    "batch_size = 1      # CPU-only, 8 GB RAM\n",
    "\n",
    "# --- Dataset / Model ---\n",
    "dataset_name = \"nielsr/funsd\"\n",
    "model_name = \"microsoft/layoutlm-base-uncased\"\n",
    "\n",
    "# --- Run Logging / Output ---\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(\"results\", \"funsd\", \"layoutlm\", \"fullrun\", run_id)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Run-Konfiguration (für Reproduzierbarkeit) ---\n",
    "run_config = {\n",
    "    \"dataset\": dataset_name,\n",
    "    \"model\": model_name,\n",
    "    \"train_split\": \"train\",\n",
    "    \"eval_split\": \"test\",      # finaler Report\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"seed\": seed,\n",
    "    \"device\": \"cpu\",\n",
    "    \"run_id\": run_id,\n",
    "    \"output_dir\": output_dir,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad17fc",
   "metadata": {},
   "source": [
    "## 1) FUNSD laden, Felder prüfen, Label-Liste erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1dad8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image'],\n",
      "        num_rows: 149\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'words', 'bboxes', 'ner_tags', 'image'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "Using token field: words\n",
      "Number of labels: 7\n",
      "First labels: ['O', 'B-HEADER', 'I-HEADER', 'B-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER']\n"
     ]
    }
   ],
   "source": [
    "# --- Load dataset ---\n",
    "funsd = load_dataset(dataset_name)\n",
    "print(funsd)\n",
    "\n",
    "# --- Full Run: use full splits (no subsetting) ---\n",
    "train_split = \"train\"\n",
    "eval_split = \"test\"  # final reporting split; for development use \"validation\"\n",
    "\n",
    "run_config[\"train_split\"] = train_split\n",
    "run_config[\"eval_split\"] = eval_split\n",
    "\n",
    "# Full run = take the entire split (no .select(range(...)))\n",
    "train_raw = funsd[train_split]\n",
    "eval_raw = funsd[eval_split]\n",
    "\n",
    "# FUNSD can use different token field names depending on dataset version\n",
    "sample = train_raw[0]\n",
    "token_field = \"words\" if \"words\" in sample else \"tokens\"\n",
    "print(\"Using token field:\", token_field)\n",
    "\n",
    "# Label set (BIO tags) as strings\n",
    "label_list = funsd[train_split].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(\"Number of labels:\", num_labels)\n",
    "print(\"First labels:\", label_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6732e15",
   "metadata": {},
   "source": [
    "## 2) Tokenizer/Model laden + Label-Mapping sauber setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d126115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakoernig/multimodal-document-ai-evaluation/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LayoutLMTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "model = LayoutLMForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# Sehr hilfreich: id2label/label2id für saubere Logs & spätere Analyse setzen\n",
    "id2label = {i: lab for i, lab in enumerate(label_list)}\n",
    "label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0daa292",
   "metadata": {},
   "source": [
    "## 3) Encoding-Funktion (WICHTIG: korrektes Subtoken-Alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f12f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_funsd_examples(raw_ds, tokenizer, token_field, max_length=None):\n",
    "    \"\"\"\n",
    "    Encode FUNSD examples for LayoutLM (FULL RUN).\n",
    "\n",
    "    Produces token-level inputs:\n",
    "    - input_ids, attention_mask\n",
    "    - bbox (token-level bounding boxes)\n",
    "    - labels (token-level labels, BIO)\n",
    "\n",
    "    Important:\n",
    "    - Only the FIRST subtoken of each word receives a label\n",
    "    - All subsequent subtokens are assigned -100\n",
    "    - Required for correct entity-level (seqeval) evaluation\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "\n",
    "    for example in raw_ds:\n",
    "        words = example[token_field]\n",
    "        word_boxes = example[\"bboxes\"]\n",
    "        word_labels = example[\"ner_tags\"]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            words,\n",
    "            truncation=True,\n",
    "            padding=False,          # dynamic padding via DataCollator\n",
    "            is_split_into_words=True,\n",
    "            max_length=max_length,  # None = no hard limit (model default, typically 512)\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "\n",
    "        previous_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # Special tokens (e.g., [CLS], [SEP])\n",
    "                labels.append(-100)\n",
    "                bboxes.append([0, 0, 0, 0])\n",
    "            else:\n",
    "                # Robust bbox handling (int + clamp to [0,1000])\n",
    "                x0, y0, x1, y1 = word_boxes[word_id]\n",
    "                x0 = int(max(0, min(1000, x0)))\n",
    "                y0 = int(max(0, min(1000, y0)))\n",
    "                x1 = int(max(0, min(1000, x1)))\n",
    "                y1 = int(max(0, min(1000, y1)))\n",
    "                bboxes.append([x0, y0, x1, y1])\n",
    "\n",
    "                # Label only for first subtoken of each word\n",
    "                if word_id != previous_word_id:\n",
    "                    labels.append(word_labels[word_id])\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        item = {k: encoding[k] for k in encoding.keys()}\n",
    "        item[\"bbox\"] = bboxes\n",
    "        item[\"labels\"] = labels\n",
    "        encoded.append(item)\n",
    "\n",
    "    return Dataset.from_list(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcabc4e",
   "metadata": {},
   "source": [
    "## 4) Datasets erzeugen + Torch-Format setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98e56da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels'],\n",
      "    num_rows: 149\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = encode_funsd_examples(train_raw, tokenizer, token_field)\n",
    "eval_dataset  = encode_funsd_examples(eval_raw, tokenizer, token_field)\n",
    "\n",
    "# Trainer arbeitet gut mit Torch-Tensors\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2228c2b0",
   "metadata": {},
   "source": [
    "## 5) seqeval compute_metrics (entity-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a3ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Entity-level Evaluation (seqeval):\n",
    "    - ignoriert -100 Labels\n",
    "    - mappt label IDs -> Label Strings (BIO tags)\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(preds, labels):\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            seq_true.append(label_list[int(l)])\n",
    "            seq_pred.append(label_list[int(p)])\n",
    "        true_labels.append(seq_true)\n",
    "        true_preds.append(seq_pred)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e46b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_raw), len(eval_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb4bbb",
   "metadata": {},
   "source": [
    "## 6) TrainingArguments (CPU-only, notebook-stabil) + Trainer + Train/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2ef416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a55aae4914451497ea093e5af7862b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7459, 'grad_norm': 7.434628963470459, 'learning_rate': 4.9378109452736324e-05, 'epoch': 0.34}\n",
      "{'loss': 1.0034, 'grad_norm': 8.642791748046875, 'learning_rate': 4.3159203980099506e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21971e3de324c8083ebfb00023f272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6608246564865112, 'eval_precision': 0.6719404374127501, 'eval_recall': 0.7245358755644756, 'eval_f1': 0.6972477064220183, 'eval_runtime': 20.2777, 'eval_samples_per_second': 2.466, 'eval_steps_per_second': 2.466, 'epoch': 1.0}\n",
      "{'loss': 0.8809, 'grad_norm': 5.578529357910156, 'learning_rate': 3.694029850746269e-05, 'epoch': 1.01}\n",
      "{'loss': 0.5805, 'grad_norm': 8.722719192504883, 'learning_rate': 3.0721393034825876e-05, 'epoch': 1.34}\n",
      "{'loss': 0.5853, 'grad_norm': 4.068142890930176, 'learning_rate': 2.4502487562189054e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f640a225866490b8db54a8699211c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6193162798881531, 'eval_precision': 0.7001375515818432, 'eval_recall': 0.7661816357250376, 'eval_f1': 0.7316722568279826, 'eval_runtime': 20.2539, 'eval_samples_per_second': 2.469, 'eval_steps_per_second': 2.469, 'epoch': 2.0}\n",
      "{'loss': 0.6393, 'grad_norm': 5.557031631469727, 'learning_rate': 1.828358208955224e-05, 'epoch': 2.01}\n",
      "{'loss': 0.4071, 'grad_norm': 2.3904988765716553, 'learning_rate': 1.2064676616915425e-05, 'epoch': 2.35}\n",
      "{'loss': 0.374, 'grad_norm': 7.742086410522461, 'learning_rate': 5.845771144278607e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d16caac283434180bac459d82b5131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6192845106124878, 'eval_precision': 0.7162855809612693, 'eval_recall': 0.77019568489714, 'eval_f1': 0.7422630560928433, 'eval_runtime': 20.4779, 'eval_samples_per_second': 2.442, 'eval_steps_per_second': 2.442, 'epoch': 3.0}\n",
      "{'train_runtime': 928.9561, 'train_samples_per_second': 0.481, 'train_steps_per_second': 0.481, 'train_loss': 0.7303057484978798, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eff50e4ddc46bfac9a6a6e4a6df848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.6192845106124878, 'eval_precision': 0.7162855809612693, 'eval_recall': 0.77019568489714, 'eval_f1': 0.7422630560928433, 'eval_runtime': 20.3556, 'eval_samples_per_second': 2.456, 'eval_steps_per_second': 2.456, 'epoch': 3.0}\n",
      "Train result: TrainOutput(global_step=447, training_loss=0.7303057484978798, metrics={'train_runtime': 928.9561, 'train_samples_per_second': 0.481, 'train_steps_per_second': 0.481, 'train_loss': 0.7303057484978798, 'epoch': 3.0})\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Training & Evaluation\n",
    "# =========================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # --- Full Run Parameters ---\n",
    "    num_train_epochs=epochs,                # z. B. 1 (Light) oder 3 (Final)\n",
    "    per_device_train_batch_size=batch_size, # empfohlen: 1\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    # --- Optimization (explizit für Reproduzierbarkeit) ---\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # --- Evaluation & Saving ---\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",          # wir speichern Metriken/Configs selbst\n",
    "    \n",
    "    # --- Logging (CPU-/Notebook-freundlich) ---\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,            # weniger Overhead als 10\n",
    "    disable_tqdm=False,          # Fortschritt sichtbar (okay über Nacht)\n",
    "\n",
    "    # --- Reproducibility ---\n",
    "    seed=seed,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    # --- Notebook / macOS Stability ---\n",
    "    dataloader_num_workers=0,    # verhindert Fork-Probleme\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # --- Device ---\n",
    "    use_cpu=True,                # explizit CPU (statt no_cuda=True)\n",
    ")\n",
    "\n",
    "# --- Dynamic padding (entscheidend für CPU-Performance) ---\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,  # pad to longest sequence in batch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "train_result = trainer.train()\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Eval metrics:\", eval_metrics)\n",
    "print(\"Train result:\", train_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd7b9a",
   "metadata": {},
   "source": [
    "## 7) Artefakte speichern (metrics.json + run_config.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347a6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved run_config.json and metrics.json to: results/funsd/layoutlm/fullrun/20260111_205332\n"
     ]
    }
   ],
   "source": [
    "# run_config erweitern um relevante Infos\n",
    "run_config.update({\n",
    "    \"num_labels\": num_labels,\n",
    "    \"label_list_preview\": label_list[:10],\n",
    "    \"device\": \"cpu\",\n",
    "})\n",
    "\n",
    "with open(os.path.join(output_dir, \"run_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_config, f, indent=2)\n",
    "\n",
    "with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_metrics, f, indent=2)\n",
    "\n",
    "print(f\"Saved run_config.json and metrics.json to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41446912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _to_device(batch, device):\n",
    "    \"\"\"Move tensors in batch dict to device.\"\"\"\n",
    "    out = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            out[k] = v.to(device)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_single_example(model, collator, features, device):\n",
    "    \"\"\"\n",
    "    Run model forward pass on one encoded example (features dict of torch tensors).\n",
    "    Returns logits (seq_len, num_labels) and label_ids (seq_len,) if present.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch = collator([features])\n",
    "    batch = _to_device(batch, device)\n",
    "    outputs = model(**{k: v for k, v in batch.items() if k != \"labels\"})\n",
    "    logits = outputs.logits[0].detach().cpu().numpy()\n",
    "    labels = batch.get(\"labels\", None)\n",
    "    if labels is not None:\n",
    "        labels = labels[0].detach().cpu().numpy()\n",
    "    return logits, labels\n",
    "\n",
    "def decode_token_level_to_word_level(tokenizer_or_processor, words, word_boxes, token_logits, token_label_ids, label_list, max_length=512):\n",
    "    \"\"\"\n",
    "    Maps token predictions to word-level predictions using word_ids.\n",
    "    - Keeps first subtoken prediction per word (like your encoding)\n",
    "    - Ignores special tokens / -100 labels\n",
    "    Returns: list of dicts per word: {word, true_label, pred_label, box}\n",
    "    \"\"\"\n",
    "    # tokenizer in LayoutLMv3 requires boxes\n",
    "    try:\n",
    "        tok = tokenizer_or_processor(\n",
    "            words,\n",
    "            boxes=word_boxes,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        word_ids = tok.word_ids()\n",
    "    except TypeError:\n",
    "        # LayoutLM tokenizer: boxes not required\n",
    "        tok = tokenizer_or_processor(\n",
    "            words,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        word_ids = tok.word_ids()\n",
    "\n",
    "    token_pred_ids = np.argmax(token_logits, axis=-1)\n",
    "\n",
    "    rows = []\n",
    "    seen_word_ids = set()\n",
    "    for t_idx, w_id in enumerate(word_ids):\n",
    "        if w_id is None:\n",
    "            continue\n",
    "        if w_id in seen_word_ids:\n",
    "            continue  # only first subtoken\n",
    "        seen_word_ids.add(w_id)\n",
    "\n",
    "        true_lab = None\n",
    "        if token_label_ids is not None:\n",
    "            true_id = int(token_label_ids[t_idx])\n",
    "            if true_id == -100:\n",
    "                true_lab = None\n",
    "            else:\n",
    "                true_lab = label_list[true_id]\n",
    "\n",
    "        pred_lab = label_list[int(token_pred_ids[t_idx])]\n",
    "        rows.append({\n",
    "            \"word_id\": w_id,\n",
    "            \"word\": words[w_id],\n",
    "            \"true_label\": true_lab,\n",
    "            \"pred_label\": pred_lab,\n",
    "            \"bbox\": word_boxes[w_id],\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "def format_ner_df(rows):\n",
    "    df = pd.DataFrame(rows)\n",
    "    # optional: show only rows with entities or mismatches\n",
    "    df[\"is_entity_true\"] = df[\"true_label\"].fillna(\"O\").ne(\"O\")\n",
    "    df[\"is_entity_pred\"] = df[\"pred_label\"].ne(\"O\")\n",
    "    df[\"mismatch\"] = (df[\"true_label\"].fillna(\"O\") != df[\"pred_label\"])\n",
    "    return df\n",
    "\n",
    "def pick_examples(eval_raw, n=3, strategy=\"mixed\"):\n",
    "    \"\"\"\n",
    "    Picks indices for qualitative analysis.\n",
    "    strategy:\n",
    "      - \"first\": first n\n",
    "      - \"random\": random n\n",
    "      - \"mixed\": mix of short/long docs by number of words\n",
    "    \"\"\"\n",
    "    if strategy == \"first\":\n",
    "        return list(range(min(n, len(eval_raw))))\n",
    "\n",
    "    if strategy == \"random\":\n",
    "        rng = np.random.default_rng(42)\n",
    "        return rng.choice(len(eval_raw), size=min(n, len(eval_raw)), replace=False).tolist()\n",
    "\n",
    "    # mixed: choose short, medium, long\n",
    "    lengths = []\n",
    "    for i in range(len(eval_raw)):\n",
    "        ex = eval_raw[i]\n",
    "        words = ex[\"words\"] if \"words\" in ex else ex[\"tokens\"]\n",
    "        lengths.append((i, len(words)))\n",
    "    lengths.sort(key=lambda x: x[1])\n",
    "    if len(lengths) == 0:\n",
    "        return []\n",
    "\n",
    "    picks = []\n",
    "    picks.append(lengths[0][0])                        # shortest\n",
    "    picks.append(lengths[len(lengths)//2][0])         # median\n",
    "    if len(lengths) > 2:\n",
    "        picks.append(lengths[-1][0])                  # longest\n",
    "    return picks[:min(n, len(picks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84db7088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected eval example indices: [26, 5, 45]\n",
      "[26] saved: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_26.csv\n",
      "[26] saved mismatches: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_26_mismatch.csv\n",
      "[5] saved: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_5.csv\n",
      "[5] saved mismatches: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_5_mismatch.csv\n",
      "[45] saved: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_45.csv\n",
      "[45] saved mismatches: results/funsd/layoutlm/fullrun/20260111_205332/qualitative/qual_example_45_mismatch.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Qualitative examples (per model)\n",
    "# =========================\n",
    "\n",
    "# 1) Set these two based on your notebook:\n",
    "# LayoutLM notebook:\n",
    "#   ENCODE_FN = encode_funsd_examples\n",
    "#   TOKENIZER_FOR_WORDIDS = tokenizer\n",
    "#\n",
    "# LayoutLMv3 notebook:\n",
    "#   ENCODE_FN = encode_funsd_examples_layoutlmv3\n",
    "#   TOKENIZER_FOR_WORDIDS = processor.tokenizer\n",
    "\n",
    "ENCODE_FN = encode_funsd_examples\n",
    "TOKENIZER_FOR_WORDIDS = tokenizer\n",
    "\n",
    "assert ENCODE_FN is not None, \"Please set ENCODE_FN to your encoding function.\"\n",
    "assert TOKENIZER_FOR_WORDIDS is not None, \"Please set TOKENIZER_FOR_WORDIDS (tokenizer or processor.tokenizer).\"\n",
    "\n",
    "device = trainer.args.device if \"trainer\" in globals() else torch.device(\"cpu\")\n",
    "model_for_pred = trainer.model if \"trainer\" in globals() else model\n",
    "\n",
    "# Use the same collator you trained with:\n",
    "# LayoutLM: DataCollatorForTokenClassification(...)\n",
    "# LayoutLMv3: default_data_collator\n",
    "collator_for_pred = trainer.data_collator if \"trainer\" in globals() else data_collator\n",
    "\n",
    "# Select 3 examples from eval_raw (short/median/long)\n",
    "example_indices = pick_examples(eval_raw, n=3, strategy=\"mixed\")\n",
    "print(\"Selected eval example indices:\", example_indices)\n",
    "\n",
    "qual_dir = os.path.join(output_dir, \"qualitative\")\n",
    "os.makedirs(qual_dir, exist_ok=True)\n",
    "\n",
    "for idx in example_indices:\n",
    "    raw_ex = eval_raw[idx]\n",
    "    words = raw_ex[token_field]\n",
    "    word_boxes = raw_ex[\"bboxes\"] if \"bboxes\" in raw_ex else raw_ex[\"bbox\"]\n",
    "\n",
    "    # Encode THIS single example using your model-specific encoding function\n",
    "    encoded_ds = ENCODE_FN([raw_ex], processor if \"processor\" in globals() else tokenizer, token_field) \\\n",
    "        if \"processor\" in globals() else ENCODE_FN([raw_ex], tokenizer, token_field)\n",
    "\n",
    "    # Your ENCODE_FN may return a Dataset; take first row as features\n",
    "    if isinstance(encoded_ds, Dataset):\n",
    "        features = encoded_ds[0]\n",
    "        # ensure torch tensors\n",
    "        # if not already tensors, Trainer collator can handle lists/np; but safer:\n",
    "        for k, v in list(features.items()):\n",
    "            if isinstance(v, np.ndarray):\n",
    "                features[k] = torch.tensor(v)\n",
    "            elif isinstance(v, list):\n",
    "                features[k] = torch.tensor(v)\n",
    "    else:\n",
    "        # If ENCODE_FN returns list of dicts\n",
    "        features = encoded_ds[0]\n",
    "\n",
    "    logits, label_ids = predict_single_example(model_for_pred, collator_for_pred, features, device)\n",
    "\n",
    "    # Decode token-level to word-level using word_ids\n",
    "    rows = decode_token_level_to_word_level(\n",
    "        TOKENIZER_FOR_WORDIDS,\n",
    "        words=words,\n",
    "        word_boxes=word_boxes,\n",
    "        token_logits=logits,\n",
    "        token_label_ids=label_ids,\n",
    "        label_list=label_list,\n",
    "        max_length=512,\n",
    "    )\n",
    "    df = format_ner_df(rows)\n",
    "\n",
    "    # Save full table + mismatch-only table\n",
    "    out_csv = os.path.join(qual_dir, f\"qual_example_{idx}.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    mismatch_df = df[df[\"mismatch\"] == True].copy()\n",
    "    out_csv_mismatch = os.path.join(qual_dir, f\"qual_example_{idx}_mismatch.csv\")\n",
    "    mismatch_df.to_csv(out_csv_mismatch, index=False)\n",
    "\n",
    "    print(f\"[{idx}] saved: {out_csv}\")\n",
    "    print(f\"[{idx}] saved mismatches: {out_csv_mismatch}\")\n",
    "\n",
    "# Also save which indices were used\n",
    "with open(os.path.join(qual_dir, \"selected_example_indices.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"indices\": example_indices}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dce897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             model       dataset train_split eval_split  \\\n",
      "0  microsoft/layoutlm-base-uncased  nielsr/funsd       train       test   \n",
      "\n",
      "   epochs  batch_size  seed device  eval_precision  eval_recall   eval_f1  \\\n",
      "0       3           1    42    cpu        0.716286     0.770196  0.742263   \n",
      "\n",
      "   eval_loss  eval_runtime  \n",
      "0   0.619285       20.3556  \n",
      "Saved: results/funsd/layoutlm/fullrun/20260111_205332/summary.json\n",
      "Saved: results/funsd/layoutlm/fullrun/20260111_205332/summary.csv\n",
      "Saved: results/funsd/layoutlm/fullrun/20260111_205332/environment.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# End-of-run summary + environment logging\n",
    "# =========================\n",
    "\n",
    "import platform\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "summary = {\n",
    "    \"model\": run_config.get(\"model\", None),\n",
    "    \"dataset\": run_config.get(\"dataset\", None),\n",
    "    \"train_split\": run_config.get(\"train_split\", None),\n",
    "    \"eval_split\": run_config.get(\"eval_split\", None),\n",
    "    \"epochs\": run_config.get(\"epochs\", None),\n",
    "    \"batch_size\": run_config.get(\"batch_size\", None),\n",
    "    \"seed\": run_config.get(\"seed\", None),\n",
    "    \"device\": run_config.get(\"device\", \"cpu\"),\n",
    "}\n",
    "\n",
    "# Pull the key metrics from eval_metrics\n",
    "summary.update({\n",
    "    \"eval_precision\": float(eval_metrics.get(\"eval_precision\", eval_metrics.get(\"precision\", np.nan))),\n",
    "    \"eval_recall\": float(eval_metrics.get(\"eval_recall\", eval_metrics.get(\"recall\", np.nan))),\n",
    "    \"eval_f1\": float(eval_metrics.get(\"eval_f1\", eval_metrics.get(\"f1\", np.nan))),\n",
    "    \"eval_loss\": float(eval_metrics.get(\"eval_loss\", np.nan)),\n",
    "    \"eval_runtime\": float(eval_metrics.get(\"eval_runtime\", np.nan)),\n",
    "})\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "print(summary_df)\n",
    "\n",
    "# Save summary files\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "summary_path_json = os.path.join(output_dir, \"summary.json\")\n",
    "summary_path_csv  = os.path.join(output_dir, \"summary.csv\")\n",
    "\n",
    "with open(summary_path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "summary_df.to_csv(summary_path_csv, index=False)\n",
    "\n",
    "print(\"Saved:\", summary_path_json)\n",
    "print(\"Saved:\", summary_path_csv)\n",
    "\n",
    "# Environment logging (very helpful for reproducibility)\n",
    "env = {\n",
    "    \"python_version\": platform.python_version(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"transformers_version\": transformers.__version__,\n",
    "    \"datasets_version\": datasets.__version__,\n",
    "}\n",
    "\n",
    "env_path = os.path.join(output_dir, \"environment.json\")\n",
    "with open(env_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", env_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
